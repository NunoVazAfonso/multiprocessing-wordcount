{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcd8b74",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Write a python program that takes as input the name of a txt file and creates another file\n",
    "having the number of occurrences of each word in the original file in descending order.   \n",
    "E.g.:  \n",
    "the 563  \n",
    "of 431  \n",
    "to 320  \n",
    "it 210  \n",
    "that 109  \n",
    "...  \n",
    "Your program should distribute the computation by having 10 worker threads simultaneously\n",
    "building the resulting list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ede5b",
   "metadata": {},
   "source": [
    "### Approach\n",
    "In this exercise, I follow a comparison between multiprocessing, multithreaded and serial approach to calculate text processing time.   \n",
    "I avoid using libraries such as Pandas and try to find the most similar approach between each methodology for accurate comparison.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacf8a8",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0816c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import re, string\n",
    "from collections import Counter\n",
    "import os\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e35fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_threads = 10\n",
    "#text_filepath=\"input/sample100.txt\"\n",
    "text_filepath=\"input/sample20.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf51ab",
   "metadata": {},
   "source": [
    "#### Functions for Multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529e216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_text( text ):\n",
    "    \"\"\"\n",
    "    Clean text from punctuation and normalize words to lower\n",
    "    \n",
    "    Params: \n",
    "        text (str): a string of text\n",
    "    \n",
    "    Returns:   \n",
    "        clean_text (List[str]): list of all words in the text sanitized \n",
    "    \"\"\"\n",
    "    lower_text = text.lower().split(' ')\n",
    "    clean_text = [re.sub('[\\W_]+', '', word) for word in lower_text ]\n",
    "    return clean_text\n",
    "    \n",
    "    \n",
    "def chunk_it( filepath, nr_chunks ) :\n",
    "    \"\"\"\n",
    "    Divide text file into chunks by line to distribute to processing nodes.\n",
    "    \n",
    "    Params: \n",
    "        filepath (str): absolute path of .txt file\n",
    "        nr_chunks (int): number of partitions to divide text into \n",
    "    \n",
    "    Returns:\n",
    "        line_chunks (List[List[str]]) : A nested list of strings containing the lines for each node to process.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(filepath, \"r\")\n",
    "    sample_text = file.readlines()\n",
    "    \n",
    "    import math\n",
    "    line_length = math.ceil(len(sample_text) / nr_chunks)\n",
    "\n",
    "    line_chunks = []\n",
    "    \n",
    "    for i in range(nr_chunks):\n",
    "        line_chunks.append(sample_text[i*line_length:(i+1)*line_length])\n",
    "    \n",
    "    print( \"We have {} chunks to process\".format( len(line_chunks) ) )\n",
    "    \n",
    "    return line_chunks\n",
    "\n",
    "\n",
    "def count_occurrences( lines ):\n",
    "    \"\"\"\n",
    "    Clean text and count each word in the text, iterating per line provided.\n",
    "    \n",
    "    Params: \n",
    "        lines (List[str]): list containing the lines within the text\n",
    "        \n",
    "    Returns:\n",
    "        count (str, int): count of occurrences per word within the chunk of text \n",
    "    \"\"\"\n",
    "    \n",
    "    print( \"Process running: \" + str(os.getpid()) + \"\\n\" ) # print process id\n",
    "\n",
    "    cleansed = []\n",
    "    for line in lines:\n",
    "        clean_line = cleanse_text(line)\n",
    "        for word in clean_line:\n",
    "            cleansed.append(word)\n",
    "        \n",
    "    count = Counter(cleansed).most_common()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1909a",
   "metadata": {},
   "source": [
    "#### Functions for regular count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abafaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences_single( text ):\n",
    "    \"\"\"\n",
    "    Cleanse and count words within provided text.\n",
    "    \n",
    "    Params:\n",
    "        text (str): the text to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        count (str, int): the count of occurrences per word\n",
    "    \"\"\"\n",
    "    \n",
    "    print( \"Process running: \" + str(os.getpid()) + \"\\n\" ) # print process id\n",
    "\n",
    "    cleansed = []\n",
    "    clean_text = cleanse_text(text)\n",
    "    for word in clean_text:\n",
    "        cleansed.append(word)\n",
    "        \n",
    "    count = Counter(cleansed).most_common()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88bb6b",
   "metadata": {},
   "source": [
    "#### Execute and time single thread (serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d63232b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process running: 36\n",
      "\n",
      "CPU times: user 5.92 s, sys: 104 ms, total: 6.03 s\n",
      "Wall time: 6.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "In this section we execute and time the regular counting process\n",
    "to establish a baseline.\n",
    "\"\"\"\n",
    "file = open(text_filepath, \"r\")\n",
    "sample_text = file.read()\n",
    "word_count = count_occurrences_single( sample_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d78c9a",
   "metadata": {},
   "source": [
    "#### Execute and time multi process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443fc49",
   "metadata": {},
   "source": [
    "Followed a similar approach to map and reduce in order to achieve paralellization accross 10 processes: \n",
    "1. Divide data to distribute to each node;\n",
    "2. Count and aggregate in each separate process;\n",
    "3. Aggregate sum results in main process;\n",
    "4. Sort and save in destination;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f11dcae4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process running: 634\n",
      "\n",
      "Process running: 635\n",
      "\n",
      "Process running: 636\n",
      "\n",
      "Process running: 637\n",
      "\n",
      "Process running: 638\n",
      "Process running: 639\n",
      "Process running: 640\n",
      "\n",
      "\n",
      "Process running: 642\n",
      "Process running: 641\n",
      "\n",
      "\n",
      "\n",
      "Process running: 643\n",
      "\n",
      "We have 10 chunks to process\n",
      "CPU times: user 45.4 ms, sys: 112 ms, total: 157 ms\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main multiprocessing function to divide text according number of partitions, \n",
    "    map to each count process and \n",
    "    reduce by summing occurrences per word.\n",
    "    \"\"\"\n",
    "    lines_array = chunk_it( text_filepath, nr_threads)\n",
    "    results = pool.map(count_occurrences, lines_array)\n",
    "        \n",
    "    words = [ entry for line in results for entry in line]\n",
    "\n",
    "    sorted_words = groupby(sorted(words), key = lambda x: x[0])\n",
    "    \n",
    "    word_aggregate = [ [key, sum(value for _, value in group)] for key, group in sorted_words ]\n",
    "    \n",
    "    sorted_counts = sorted(word_aggregate, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(processes=nr_threads)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f106756",
   "metadata": {},
   "source": [
    "#### Multithreaded approach\n",
    "To compare with previous approach - subject to GIL and not truly parallel processing, only multithread (all in same process).  \n",
    "https://medium.com/contentsquare-engineering-blog/multithreading-vs-multiprocessing-in-python-ece023ad55a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "218d6f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 chunks to process\n",
      "Process running: 36\n",
      "Process running: 36\n",
      "\n",
      "\n",
      "Process running: 36\n",
      "\n",
      "Process running: 36\n",
      "Process running: 36\n",
      "\n",
      "\n",
      "Process running: 36\n",
      "\n",
      "Process running: 36\n",
      "\n",
      "Process running: 36\n",
      "\n",
      "Process running: 36\n",
      "\n",
      "Process running: 36\n",
      "\n",
      "CPU times: user 9.26 s, sys: 203 ms, total: 9.47 s\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import concurrent.futures\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Multithreading approach function to compare with prior .\n",
    "    \"\"\"\n",
    "    lines_array = chunk_it( text_filepath, nr_threads)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = executor.map(count_occurrences, lines_array)\n",
    "\n",
    "    words = [ entry for line in results for entry in line]\n",
    "\n",
    "    sorted_words = groupby(sorted(words), key = lambda x: x[0])\n",
    "    \n",
    "    word_aggregate = [ [key, sum(value for _, value in group)] for key, group in sorted_words ]\n",
    "    \n",
    "    sorted_counts = sorted(word_aggregate, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de91d27",
   "metadata": {},
   "source": [
    "#### Write wordcount file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file ( destination_filepath , wordcount_list )\n",
    "    with open( destination_filepath, 'w+') as f:\n",
    "        for word, value in wordcount_list:\n",
    "            f.write(\"{} , {}\\n\".format(word, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f8e14",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The serial (regular) approach served as the baseline, and for text processing the Multiprocess was considerably faster, due to paralellism and load distribution.  \n",
    "On the other hand, Multithread approach is detrimental in this case - better for concurrency (IO problems), not paralellism, single process. It even adds overhead when compared to baseline, due to thread spawn and communication.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
